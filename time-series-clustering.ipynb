{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T16:04:38.281413Z","iopub.execute_input":"2022-06-16T16:04:38.282538Z","iopub.status.idle":"2022-06-16T16:04:38.341009Z","shell.execute_reply.started":"2022-06-16T16:04:38.282396Z","shell.execute_reply":"2022-06-16T16:04:38.340199Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install MiniSom","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:09:49.073712Z","iopub.execute_input":"2022-06-16T16:09:49.074258Z","iopub.status.idle":"2022-06-16T16:10:04.254157Z","shell.execute_reply.started":"2022-06-16T16:09:49.074212Z","shell.execute_reply":"2022-06-16T16:10:04.252854Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!python -m pip install tslearn","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:10:56.201392Z","iopub.execute_input":"2022-06-16T16:10:56.201835Z","iopub.status.idle":"2022-06-16T16:11:07.725545Z","shell.execute_reply.started":"2022-06-16T16:10:56.201804Z","shell.execute_reply":"2022-06-16T16:11:07.724288Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom minisom import MiniSom\nfrom tslearn.barycenters import dtw_barycenter_averaging\nfrom tslearn.clustering import TimeSeriesKMeans\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.decomposition import PCA","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:11:27.269355Z","iopub.execute_input":"2022-06-16T16:11:27.269821Z","iopub.status.idle":"2022-06-16T16:11:28.961077Z","shell.execute_reply.started":"2022-06-16T16:11:27.269782Z","shell.execute_reply":"2022-06-16T16:11:28.960017Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"directory = '/kaggle/input/retail-and-retailers-sales-time-series-collection/'\n\nmySeries = []\nnamesofMySeries = []\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\"):\n        df = pd.read_csv(directory+filename)\n        df = df.loc[:,[\"date\",\"value\"]]\n        # While we are at it I just filtered the columns that we will be working on\n        df.set_index(\"date\",inplace=True)\n        # ,set the date columns as index\n        df.sort_index(inplace=True)\n        # and lastly, ordered the data according to our date index\n        mySeries.append(df)\n        namesofMySeries.append(filename[:-4])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:12:15.417236Z","iopub.execute_input":"2022-06-16T16:12:15.417643Z","iopub.status.idle":"2022-06-16T16:12:15.713485Z","shell.execute_reply.started":"2022-06-16T16:12:15.417611Z","shell.execute_reply":"2022-06-16T16:12:15.712482Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(len(mySeries))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:12:33.262480Z","iopub.execute_input":"2022-06-16T16:12:33.262886Z","iopub.status.idle":"2022-06-16T16:12:33.268180Z","shell.execute_reply.started":"2022-06-16T16:12:33.262855Z","shell.execute_reply":"2022-06-16T16:12:33.267108Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# So, for 23 series let's create a 6 by 4 grid which will be resulted in 24 slots and fill it with the plot of our series.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(6,4,figsize=(25,25))\nfig.suptitle('Series')\nfor i in range(6):\n    for j in range(4):\n        if i*4+j+1>len(mySeries): # pass the others that we can't fill\n            continue\n        axs[i, j].plot(mySeries[i*4+j].values)\n        axs[i, j].set_title(namesofMySeries[i*4+j])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:13:19.785390Z","iopub.execute_input":"2022-06-16T16:13:19.785854Z","iopub.status.idle":"2022-06-16T16:13:22.939986Z","shell.execute_reply.started":"2022-06-16T16:13:19.785820Z","shell.execute_reply":"2022-06-16T16:13:22.938973Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(6,4,figsize=(25,25))\nfig.suptitle('Series')\nfor i in range(6):\n    for j in range(4):\n        if i*4+j+1>len(mySeries): # pass the others that we can't fill\n            continue\n        axs[i, j].plot(mySeries[i*4+j].values)\n        axs[i, j].set_title(namesofMySeries[i*4+j])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:14:16.434899Z","iopub.execute_input":"2022-06-16T16:14:16.435299Z","iopub.status.idle":"2022-06-16T16:14:19.651502Z","shell.execute_reply.started":"2022-06-16T16:14:16.435267Z","shell.execute_reply":"2022-06-16T16:14:19.650496Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing.","metadata":{}},{"cell_type":"code","source":"# let's check if our data is uniform in length.\nseries_lengths = {len(series) for series in mySeries}\nprint(series_lengths)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:18:07.628613Z","iopub.execute_input":"2022-06-16T16:18:07.629096Z","iopub.status.idle":"2022-06-16T16:18:07.634691Z","shell.execute_reply.started":"2022-06-16T16:18:07.629062Z","shell.execute_reply":"2022-06-16T16:18:07.633844Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**It is not uniform in length. So in this case, we should find which series contain missing data and fill them. Because, otherwise our indices will be shifted and i.th index -let's say it is 10th of May- of the x series won't be same as i.th index of the y series -let's say i.th index of the y series may be 11th of May**","metadata":{}},{"cell_type":"code","source":"ind = 0\nfor series in mySeries:\n    print(\"[\"+str(ind)+\"] \"+series.index[0]+\" \"+series.index[len(series)-1])\n    ind+=1","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:23:29.304286Z","iopub.execute_input":"2022-06-16T16:23:29.304738Z","iopub.status.idle":"2022-06-16T16:23:29.311571Z","shell.execute_reply.started":"2022-06-16T16:23:29.304704Z","shell.execute_reply":"2022-06-16T16:23:29.310604Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"max_len = max(series_lengths)\nlongest_series = None\nfor series in mySeries:\n    if len(series) == max_len:\n        longest_series = series","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:24:15.280295Z","iopub.execute_input":"2022-06-16T16:24:15.281011Z","iopub.status.idle":"2022-06-16T16:24:15.286751Z","shell.execute_reply.started":"2022-06-16T16:24:15.280970Z","shell.execute_reply":"2022-06-16T16:24:15.285773Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# In this code block, I reindexed the series that are not as long as the longest one and fill the empty dates with np.nan.","metadata":{}},{"cell_type":"code","source":"problems_index = []\n\nfor i in range(len(mySeries)):\n    if len(mySeries[i])!= max_len:\n        problems_index.append(i)\n        mySeries[i] = mySeries[i].reindex(longest_series.index)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:25:53.134271Z","iopub.execute_input":"2022-06-16T16:25:53.135500Z","iopub.status.idle":"2022-06-16T16:25:53.142476Z","shell.execute_reply.started":"2022-06-16T16:25:53.135457Z","shell.execute_reply":"2022-06-16T16:25:53.141721Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# check how many series are polluted with nan values with this function.","metadata":{}},{"cell_type":"code","source":"def nan_counter(list_of_series):\n    nan_polluted_series_counter = 0\n    for series in list_of_series:\n        if series.isnull().sum().sum() > 0:\n            nan_polluted_series_counter+=1\n    print(nan_polluted_series_counter)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:31:02.517166Z","iopub.execute_input":"2022-06-16T16:31:02.517709Z","iopub.status.idle":"2022-06-16T16:31:02.523504Z","shell.execute_reply.started":"2022-06-16T16:31:02.517668Z","shell.execute_reply":"2022-06-16T16:31:02.522504Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"nan_counter(mySeries)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:31:19.585678Z","iopub.execute_input":"2022-06-16T16:31:19.586301Z","iopub.status.idle":"2022-06-16T16:31:19.611217Z","shell.execute_reply.started":"2022-06-16T16:31:19.586252Z","shell.execute_reply":"2022-06-16T16:31:19.609860Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Because these series lack only one point, I used linear interpolation to fill the gap but for series that have more missing value, you can use much more complex interpolation methods such as quadratic, cubic, spline, barycentric, etc.","metadata":{}},{"cell_type":"code","source":"for i in problems_index:\n    mySeries[i].interpolate(limit_direction=\"both\",inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:34:30.065213Z","iopub.execute_input":"2022-06-16T16:34:30.065742Z","iopub.status.idle":"2022-06-16T16:34:30.075835Z","shell.execute_reply.started":"2022-06-16T16:34:30.065706Z","shell.execute_reply":"2022-06-16T16:34:30.075101Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# As we can see, now all of our series are the same length and don't contain any missing value.\nnan_counter(mySeries)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:34:54.757030Z","iopub.execute_input":"2022-06-16T16:34:54.757969Z","iopub.status.idle":"2022-06-16T16:34:54.778986Z","shell.execute_reply.started":"2022-06-16T16:34:54.757932Z","shell.execute_reply":"2022-06-16T16:34:54.777823Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# After handling missing values, the other issue is the scale of the series. Without, normalizing data the series that looks like each other will be seen so different from each other and will affect the accuracy of the clustering process. We can see the effect of the normalizing in the following images.","metadata":{}},{"cell_type":"code","source":"a = [[2],[7],[11],[14],[19],[23],[26]]\nb = [[20000000],[40000000],[60000000],[80000000],[100000000],[120000000],[140000000]]\nfig, axs = plt.subplots(1,3,figsize=(25,5))\naxs[0].plot(a)\naxs[0].set_title(\"Series 1\")\naxs[1].plot(b)\naxs[1].set_title(\"Series 2\")\naxs[2].plot(a)\naxs[2].plot(b)\naxs[2].set_title(\"Series 1 & 2\")\nplt.figure(figsize=(25,5))\nplt.plot(MinMaxScaler().fit_transform(a))\nplt.plot(MinMaxScaler().fit_transform(b))\nplt.title(\"Normalized Series 1 & Series 2\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:37:01.478515Z","iopub.execute_input":"2022-06-16T16:37:01.478948Z","iopub.status.idle":"2022-06-16T16:37:02.108629Z","shell.execute_reply.started":"2022-06-16T16:37:01.478917Z","shell.execute_reply":"2022-06-16T16:37:02.107510Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# each value was normalized by its own value, not the values of other time series.","metadata":{}},{"cell_type":"code","source":"for i in range(len(mySeries)):\n    scaler = MinMaxScaler()\n    mySeries[i] = MinMaxScaler().fit_transform(mySeries[i])\n    mySeries[i]= mySeries[i].reshape(len(mySeries[i]))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:39:58.077399Z","iopub.execute_input":"2022-06-16T16:39:58.077827Z","iopub.status.idle":"2022-06-16T16:39:58.138567Z","shell.execute_reply.started":"2022-06-16T16:39:58.077795Z","shell.execute_reply":"2022-06-16T16:39:58.137614Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# The result of the normalizing process seems fine.\nprint(\"max: \"+str(max(mySeries[0]))+\"\\tmin: \"+str(min(mySeries[0])))\nprint(mySeries[0][:5])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:40:23.077518Z","iopub.execute_input":"2022-06-16T16:40:23.077901Z","iopub.status.idle":"2022-06-16T16:40:23.084404Z","shell.execute_reply.started":"2022-06-16T16:40:23.077872Z","shell.execute_reply":"2022-06-16T16:40:23.083490Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Clustering.\n**2 different methods for clustering will be used. The first of the methods is Self Organizing Maps(SOM) and the other method is K-Means.**","metadata":{}},{"cell_type":"code","source":"som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(mySeries))))\n# I didn't see its significance but to make the map square,\n# I calculated square root of map size which is \n# the square root of the number of series\n# for the row and column counts of som\n\nsom = MiniSom(som_x, som_y,len(mySeries[0]), sigma=0.3, learning_rate = 0.1)\n\nsom.random_weights_init(mySeries)\nsom.train(mySeries, 50000)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:49:28.993684Z","iopub.execute_input":"2022-06-16T16:49:28.994112Z","iopub.status.idle":"2022-06-16T16:49:32.750994Z","shell.execute_reply.started":"2022-06-16T16:49:28.994080Z","shell.execute_reply":"2022-06-16T16:49:32.749945Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Because of the ability to produce a map, SOM deemed as a method to do dimensionality reduction. But in our case, when each node of the som is accepted as medoids of the cluster, we can use it for clustering. To do so, we should remove our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point.**","metadata":{}},{"cell_type":"code","source":"# Little handy function to plot series\ndef plot_som_series_averaged_center(som_x, som_y, win_map):\n    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n    fig.suptitle('Clusters')\n    for x in range(som_x):\n        for y in range(som_y):\n            cluster = (x,y)\n            if cluster in win_map.keys():\n                for series in win_map[cluster]:\n                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n                axs[cluster].plot(np.average(np.vstack(win_map[cluster]),axis=0),c=\"red\")\n            cluster_number = x*som_y+y+1\n            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:54:07.164652Z","iopub.execute_input":"2022-06-16T16:54:07.165067Z","iopub.status.idle":"2022-06-16T16:54:07.173741Z","shell.execute_reply.started":"2022-06-16T16:54:07.165026Z","shell.execute_reply":"2022-06-16T16:54:07.172713Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"win_map = som.win_map(mySeries)\n# Returns the mapping of the winner nodes and inputs\nplot_som_series_averaged_center(som_x, som_y, win_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:54:28.536597Z","iopub.execute_input":"2022-06-16T16:54:28.536981Z","iopub.status.idle":"2022-06-16T16:54:29.729910Z","shell.execute_reply.started":"2022-06-16T16:54:28.536943Z","shell.execute_reply":"2022-06-16T16:54:29.729129Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"win_map = som.win_map(mySeries)\n# Returns the mapping of the winner nodes and inputs\nplot_som_series_averaged_center(som_x, som_y, win_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T16:54:56.542037Z","iopub.execute_input":"2022-06-16T16:54:56.542662Z","iopub.status.idle":"2022-06-16T16:54:58.009955Z","shell.execute_reply.started":"2022-06-16T16:54:56.542625Z","shell.execute_reply":"2022-06-16T16:54:58.009176Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Another method to extract the movement/shape of the cluster is instead of averaging each series in the cluster, using Dynamic Time Warping Barycenter Averaging (DBA).DBA is another type of averaging method that used the Dynamic Time Warping method in it and might be very useful to extract the movement/shape of the cluster as seen in the following images.","metadata":{}},{"cell_type":"code","source":"def plot_som_series_dba_center(som_x, som_y, win_map):\n    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n    fig.suptitle('Clusters')\n    for x in range(som_x):\n        for y in range(som_y):\n            cluster = (x,y)\n            if cluster in win_map.keys():\n                for series in win_map[cluster]:\n                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n                axs[cluster].plot(dtw_barycenter_averaging(np.vstack(win_map[cluster])),c=\"red\") # I changed this part\n            cluster_number = x*som_y+y+1\n            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:00:51.143473Z","iopub.execute_input":"2022-06-16T17:00:51.144342Z","iopub.status.idle":"2022-06-16T17:00:51.153695Z","shell.execute_reply.started":"2022-06-16T17:00:51.144294Z","shell.execute_reply":"2022-06-16T17:00:51.152765Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"win_map = som.win_map(mySeries)\n\nplot_som_series_dba_center(som_x, som_y, win_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:01:06.454452Z","iopub.execute_input":"2022-06-16T17:01:06.454835Z","iopub.status.idle":"2022-06-16T17:01:09.919576Z","shell.execute_reply.started":"2022-06-16T17:01:06.454806Z","shell.execute_reply":"2022-06-16T17:01:09.918530Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"win_map = som.win_map(mySeries)\n\nplot_som_series_dba_center(som_x, som_y, win_map)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:01:37.721143Z","iopub.execute_input":"2022-06-16T17:01:37.721582Z","iopub.status.idle":"2022-06-16T17:01:39.459818Z","shell.execute_reply.started":"2022-06-16T17:01:37.721547Z","shell.execute_reply":"2022-06-16T17:01:39.458785Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Cluster distribution","metadata":{}},{"cell_type":"code","source":"cluster_c = []\ncluster_n = []\nfor x in range(som_x):\n    for y in range(som_y):\n        cluster = (x,y)\n        if cluster in win_map.keys():\n            cluster_c.append(len(win_map[cluster]))\n        else:\n            cluster_c.append(0)\n        cluster_number = x*som_y+y+1\n        cluster_n.append(f\"Cluster {cluster_number}\")\n\nplt.figure(figsize=(25,5))\nplt.title(\"Cluster Distribution for SOM\")\nplt.bar(cluster_n,cluster_c)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:04:58.487679Z","iopub.execute_input":"2022-06-16T17:04:58.488092Z","iopub.status.idle":"2022-06-16T17:04:58.705889Z","shell.execute_reply.started":"2022-06-16T17:04:58.488064Z","shell.execute_reply":"2022-06-16T17:04:58.704805Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Let's check first 5\nfor series in mySeries[:5]:\n    print(som.winner(series))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:05:40.910579Z","iopub.execute_input":"2022-06-16T17:05:40.911371Z","iopub.status.idle":"2022-06-16T17:05:40.917688Z","shell.execute_reply.started":"2022-06-16T17:05:40.911325Z","shell.execute_reply":"2022-06-16T17:05:40.916497Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"cluster_map = []\nfor idx in range(len(mySeries)):\n    winner_node = som.winner(mySeries[idx])\n    cluster_map.append((namesofMySeries[idx],f\"Cluster {winner_node[0]*som_y+winner_node[1]+1}\"))\n\npd.DataFrame(cluster_map,columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:07:48.252601Z","iopub.execute_input":"2022-06-16T17:07:48.253037Z","iopub.status.idle":"2022-06-16T17:07:48.277645Z","shell.execute_reply.started":"2022-06-16T17:07:48.253007Z","shell.execute_reply":"2022-06-16T17:07:48.276473Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"cluster_count = math.ceil(math.sqrt(len(mySeries))) \n# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN\n\nkm = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n\nlabels = km.fit_predict(mySeries)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:08:22.865932Z","iopub.execute_input":"2022-06-16T17:08:22.867082Z","iopub.status.idle":"2022-06-16T17:08:25.365621Z","shell.execute_reply.started":"2022-06-16T17:08:22.867037Z","shell.execute_reply":"2022-06-16T17:08:25.364658Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\n# For each label there is,\n# plots every series with that label\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:09:43.424031Z","iopub.execute_input":"2022-06-16T17:09:43.424692Z","iopub.status.idle":"2022-06-16T17:09:44.794172Z","shell.execute_reply.started":"2022-06-16T17:09:43.424656Z","shell.execute_reply":"2022-06-16T17:09:44.793263Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\n# For each label there is,\n# plots every series with that label\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:11:30.473289Z","iopub.execute_input":"2022-06-16T17:11:30.473696Z","iopub.status.idle":"2022-06-16T17:11:31.620588Z","shell.execute_reply.started":"2022-06-16T17:11:30.473663Z","shell.execute_reply":"2022-06-16T17:11:31.619463Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(dtw_barycenter_averaging(np.vstack(cluster)),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:11:50.893490Z","iopub.execute_input":"2022-06-16T17:11:50.893907Z","iopub.status.idle":"2022-06-16T17:11:52.859618Z","shell.execute_reply.started":"2022-06-16T17:11:50.893874Z","shell.execute_reply":"2022-06-16T17:11:52.858571Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\ncluster_n = [\"Cluster \"+str(i) for i in range(cluster_count)]\nplt.figure(figsize=(15,5))\nplt.title(\"Cluster Distribution for KMeans\")\nplt.bar(cluster_n,cluster_c)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:12:11.259342Z","iopub.execute_input":"2022-06-16T17:12:11.260231Z","iopub.status.idle":"2022-06-16T17:12:11.447053Z","shell.execute_reply.started":"2022-06-16T17:12:11.260189Z","shell.execute_reply":"2022-06-16T17:12:11.446312Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:12:23.243489Z","iopub.execute_input":"2022-06-16T17:12:23.244135Z","iopub.status.idle":"2022-06-16T17:12:23.250184Z","shell.execute_reply.started":"2022-06-16T17:12:23.244101Z","shell.execute_reply":"2022-06-16T17:12:23.249478Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\npd.DataFrame(zip(namesofMySeries,fancy_names_for_labels),columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:12:33.026857Z","iopub.execute_input":"2022-06-16T17:12:33.027497Z","iopub.status.idle":"2022-06-16T17:12:33.040994Z","shell.execute_reply.started":"2022-06-16T17:12:33.027453Z","shell.execute_reply":"2022-06-16T17:12:33.039914Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\nmySeries_transformed = pca.fit_transform(mySeries)\nplt.figure(figsize=(25,10))\nplt.scatter(mySeries_transformed[:,0],mySeries_transformed[:,1], s=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:13:20.618171Z","iopub.execute_input":"2022-06-16T17:13:20.619122Z","iopub.status.idle":"2022-06-16T17:13:20.861537Z","shell.execute_reply.started":"2022-06-16T17:13:20.619075Z","shell.execute_reply":"2022-06-16T17:13:20.860575Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print(mySeries_transformed[0:5])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:16:14.679261Z","iopub.execute_input":"2022-06-16T17:16:14.679723Z","iopub.status.idle":"2022-06-16T17:16:14.685663Z","shell.execute_reply.started":"2022-06-16T17:16:14.679686Z","shell.execute_reply":"2022-06-16T17:16:14.684786Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=cluster_count,max_iter=5000)\n\nlabels = kmeans.fit_predict(mySeries_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:16:29.282707Z","iopub.execute_input":"2022-06-16T17:16:29.283413Z","iopub.status.idle":"2022-06-16T17:16:29.311905Z","shell.execute_reply.started":"2022-06-16T17:16:29.283370Z","shell.execute_reply":"2022-06-16T17:16:29.310985Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,10))\nplt.scatter(mySeries_transformed[:, 0], mySeries_transformed[:, 1], c=labels, s=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:16:42.159697Z","iopub.execute_input":"2022-06-16T17:16:42.160112Z","iopub.status.idle":"2022-06-16T17:16:42.376057Z","shell.execute_reply.started":"2022-06-16T17:16:42.160078Z","shell.execute_reply":"2022-06-16T17:16:42.375287Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:17:01.277112Z","iopub.execute_input":"2022-06-16T17:17:01.278201Z","iopub.status.idle":"2022-06-16T17:17:02.410949Z","shell.execute_reply.started":"2022-06-16T17:17:01.278156Z","shell.execute_reply":"2022-06-16T17:17:02.409897Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# And we can see that now with the PCA algorithm, our series are much more equally distributed to clusters than before.\n","metadata":{}},{"cell_type":"code","source":"cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\ncluster_n = [\"cluster_\"+str(i) for i in range(cluster_count)]\nplt.figure(figsize=(15,5))\nplt.title(\"Cluster Distribution for KMeans\")\nplt.bar(cluster_n,cluster_c)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:17:40.781080Z","iopub.execute_input":"2022-06-16T17:17:40.781526Z","iopub.status.idle":"2022-06-16T17:17:41.212578Z","shell.execute_reply.started":"2022-06-16T17:17:40.781492Z","shell.execute_reply":"2022-06-16T17:17:41.211511Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\npd.DataFrame(zip(namesofMySeries,fancy_names_for_labels),columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:19:21.084644Z","iopub.execute_input":"2022-06-16T17:19:21.085024Z","iopub.status.idle":"2022-06-16T17:19:21.099055Z","shell.execute_reply.started":"2022-06-16T17:19:21.084994Z","shell.execute_reply":"2022-06-16T17:19:21.098298Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}